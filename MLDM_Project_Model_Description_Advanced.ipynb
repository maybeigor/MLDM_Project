{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HBEv2W5jXk_W"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EXPLAINING AUDIO MODELS USED IN THE RESEARCH."
      ],
      "metadata": {
        "id": "-fTz-eQxXZ_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper"
      ],
      "metadata": {
        "id": "HBEv2W5jXk_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our project, we aimed at finetuning several existing audio models, one of which includes Whisper model by OpenAI.\n",
        "\n",
        "Whisper is an automatic speech recognition (ASR) system developed by OpenAI. It utilizes a deep learning architecture to convert spoken language into written text. It involves several steps:\n",
        "\n",
        "1. Data Preparation: Whisper requires a large amount of labeled audio data for training. This data is typically collected and transcribed, creating pairs of audio segments and their corresponding textual transcripts. The data is then preprocessed to extract features that capture relevant information from the audio signals, such as Mel-frequency cepstral coefficients (MFCCs) or spectrograms.\n",
        "\n",
        "2. Acoustic Modeling: Whisper utilizes a deep neural network architecture, often based on recurrent neural networks (RNNs), to model the relationship between audio features and textual representations. Long Short-Term Memory (LSTM) networks are commonly used due to their ability to capture long-term dependencies in sequential data. The audio features are fed into the network, which predicts the probability distribution over a set of linguistic units (such as phonemes or subword units) at each time step.\n",
        "\n",
        "3. Language Modeling: To improve the accuracy of transcription, Whisper incorporates a language model that adds linguistic context to the ASR system. Language models can be based on n-gram models, recurrent neural networks, or transformers. These models help resolve ambiguities and improve the overall quality of the transcriptions by considering the likelihood of certain word sequences.\n",
        "\n",
        "4. Training: The Whisper model is trained using a large amount of paired audio-text data. The training process involves optimizing the model's parameters to minimize the difference between predicted transcriptions and the ground truth transcriptions. This is typically done using gradient-based optimization algorithms such as stochastic gradient descent (SGD) or its variants.\n",
        "\n",
        "5. Decoding: Once the Whisper model is trained, it can be used for inference on new, unseen audio data. During decoding, the model takes as input the audio features and generates a sequence of predicted linguistic units. This sequence is then transformed into the final text output using decoding techniques such as the Connectionist Temporal Classification (CTC) algorithm or attention mechanisms."
      ],
      "metadata": {
        "id": "ZayIRCwXXoEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual code of Whisper model provided by OpenAI is quite long, and in this part of research we aim at analyzing each of the parts of the model code in detail."
      ],
      "metadata": {
        "id": "io8wXdlYYd1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import gzip\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from .decoding import decode as decode_function\n",
        "from .decoding import detect_language as detect_language_function\n",
        "from .transcribe import transcribe as transcribe_function"
      ],
      "metadata": {
        "id": "aCwLagMMX5nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we import all the necessary Python libraries needed to run the following model code."
      ],
      "metadata": {
        "id": "fFrQEE9wY4bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelDimensions:\n",
        "    n_mels: int\n",
        "    n_audio_ctx: int\n",
        "    n_audio_state: int\n",
        "    n_audio_head: int\n",
        "    n_audio_layer: int\n",
        "    n_vocab: int\n",
        "    n_text_ctx: int\n",
        "    n_text_state: int\n",
        "    n_text_head: int\n",
        "    n_text_layer: int\n",
        "\n",
        "\n",
        "class LayerNorm(nn.LayerNorm):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "\n",
        "class Linear(nn.Linear):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return F.linear(\n",
        "            x,\n",
        "            self.weight.to(x.dtype),\n",
        "            None if self.bias is None else self.bias.to(x.dtype),\n",
        "        )"
      ],
      "metadata": {
        "id": "-qJaiMl-Y-89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the above code:\n",
        "\n",
        "1. @dataclass: This is a decorator from the dataclass module in Python's standard library. It allows you to easily create classes that are primarily used to hold data. In this code, it is used to define the ModelDimensions class as a data class.\n",
        "\n",
        "2. ModelDimensions: This class represents the dimensions or sizes of various components of a model. It is defined using the dataclass decorator. The class has several attributes such as n_mels, n_audio_ctx, n_audio_state, and so on, which are integers representing the sizes or dimensions of different parts of the model.\n",
        "\n",
        "3. LayerNorm: This class is a subclass of nn.LayerNorm, which is a PyTorch module for performing layer normalization. The forward method of LayerNorm overrides the base class's forward method. It takes a tensor x as input, applies layer normalization to x.float() (casting it to float), and then returns the normalized tensor. The type(x.dtype) part ensures that the output tensor has the same data type as the input tensor.\n",
        "\n",
        "4. Linear: This class is a subclass of nn.Linear, which represents a linear transformation (commonly known as a fully connected or dense layer) in a neural network. The forward method of Linear overrides the base class's forward method. It takes a tensor x as input and applies a linear transformation to x using the weights and biases defined in the nn.Linear class. The F.linear function is used to perform the linear transformation, and the resulting tensor is returned."
      ],
      "metadata": {
        "id": "JYG3pxpqZmLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1d(nn.Conv1d):\n",
        "    def _conv_forward(\n",
        "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
        "    ) -> Tensor:\n",
        "        return super()._conv_forward(\n",
        "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
        "        )\n",
        "\n",
        "\n",
        "def sinusoids(length, channels, max_timescale=10000):\n",
        "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
        "    assert channels % 2 == 0\n",
        "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
        "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
        "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
        "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_state: int, n_head: int):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.query = Linear(n_state, n_state)\n",
        "        self.key = Linear(n_state, n_state, bias=False)\n",
        "        self.value = Linear(n_state, n_state)\n",
        "        self.out = Linear(n_state, n_state)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        xa: Optional[Tensor] = None,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        kv_cache: Optional[dict] = None,\n",
        "    ):\n",
        "        q = self.query(x)\n",
        "\n",
        "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
        "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
        "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
        "            k = self.key(x if xa is None else xa)\n",
        "            v = self.value(x if xa is None else xa)\n",
        "        else:\n",
        "            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
        "            k = kv_cache[self.key]\n",
        "            v = kv_cache[self.value]\n",
        "\n",
        "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
        "        return self.out(wv), qk\n",
        "\n",
        "    def qkv_attention(\n",
        "        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n",
        "    ):\n",
        "        n_batch, n_ctx, n_state = q.shape\n",
        "        scale = (n_state // self.n_head) ** -0.25\n",
        "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
        "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
        "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
        "\n",
        "        qk = q @ k\n",
        "        if mask is not None:\n",
        "            qk = qk + mask[:n_ctx, :n_ctx]\n",
        "        qk = qk.float()\n",
        "\n",
        "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
        "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()"
      ],
      "metadata": {
        "id": "NF9afnnzZ6w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided Python code defines several classes and functions related to neural network modules used in a model. Let's break down the code step by step:\n",
        "\n",
        "1. Conv1d: This class is a subclass of nn.Conv1d, which represents a 1-dimensional convolutional layer in a neural network. The _conv_forward method is implemented to override the base class's _conv_forward method. It performs the convolution operation by calling the _conv_forward method of the base class and passing the input tensor x, weight tensor weight (converted to the same data type as x), and bias tensor (if not None, converted to the same data type as x).\n",
        "\n",
        "2. Sinusoids: This function generates sinusoids for positional embedding. It takes three arguments: length (the length of the sinusoids), channels (the number of channels in the output tensor), and max_timescale (the maximum timescale value for the sinusoids). It calculates logarithmic timescale increments based on the number of channels and uses them to generate sinusoids for both sine and cosine functions. The resulting sinusoids are concatenated along the channel dimension and returned as a tensor.\n",
        "\n",
        "3. MultiHeadAttention: This class represents the multi-head attention mechanism in a neural network. It is a subclass of nn.Module. The class constructor takes two arguments: n_state (the input dimension of the attention mechanism) and n_head (the number of attention heads). The class defines several linear layers (self.query, self.key, self.value, self.out) which are used to project the input to the corresponding dimensions for attention calculations.\n",
        "\n",
        "4. Forward method: This method overrides the base class's forward method. It performs the forward pass of the multi-head attention mechanism. It takes x as the input tensor, xa as an optional auxiliary input tensor, mask as an optional mask tensor, and kv_cache as an optional dictionary used for caching key-value projections for cross-attention. It first applies the query projection (self.query) to x. Depending on the presence of kv_cache and xa, it either performs key-value projections using self.key and self.value, or retrieves them from the cache. Then, it calls the qkv_attention method to compute the attention weights and the weighted sum of values. Finally, it applies the output projection (self.out) to the weighted sum and returns the result along with the attention weights.\n",
        "\n",
        "5. qkv_attention method: This method performs the core calculations of the multi-head attention mechanism. It takes query tensor q, key tensor k, and value tensor v. It reshapes and permutes the tensors to prepare them for attention calculations. It calculates the attention scores by multiplying the query and key tensors, applies an optional mask, and performs softmax normalization. Finally, it computes the weighted sum of values using the attention scores and returns the result along with the attention scores.\n",
        "\n",
        "Overall, the code defines classes and functions related to convolutional layers (Conv1d), positional embedding generation (sinusoids), and multi-head attention mechanism (MultiHeadAttention) commonly used in neural network models."
      ],
      "metadata": {
        "id": "jLSdCTMMaS7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = MultiHeadAttention(n_state, n_head)\n",
        "        self.attn_ln = LayerNorm(n_state)\n",
        "\n",
        "        self.cross_attn = (\n",
        "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
        "        )\n",
        "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
        "\n",
        "        n_mlp = n_state * 4\n",
        "        self.mlp = nn.Sequential(\n",
        "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
        "        )\n",
        "        self.mlp_ln = LayerNorm(n_state)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        xa: Optional[Tensor] = None,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        kv_cache: Optional[dict] = None,\n",
        "    ):\n",
        "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
        "        if self.cross_attn:\n",
        "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
        "        x = x + self.mlp(self.mlp_ln(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
        "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
        "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
        "\n",
        "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
        "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln_post = LayerNorm(n_state)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        \"\"\"\n",
        "        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n",
        "            the mel spectrogram of the audio\n",
        "        \"\"\"\n",
        "        x = F.gelu(self.conv1(x))\n",
        "        x = F.gelu(self.conv2(x))\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
        "        x = (x + self.positional_embedding).to(x.dtype)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_post(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TextDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
        "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
        "\n",
        "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
        "            [\n",
        "                ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n",
        "                for _ in range(n_layer)\n",
        "            ]\n",
        "        )\n",
        "        self.ln = LayerNorm(n_state)\n",
        "\n",
        "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
        "        self.register_buffer(\"mask\", mask, persistent=False)\n",
        "\n",
        "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
        "        \"\"\"\n",
        "        x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n",
        "            the text tokens\n",
        "        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n",
        "            the encoded audio features to be attended on\n",
        "        \"\"\"\n",
        "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
        "        x = (\n",
        "            self.token_embedding(x)\n",
        "            + self.positional_embedding[offset : offset + x.shape[-1]]\n",
        "        )\n",
        "        x = x.to(xa.dtype)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
        "\n",
        "        x = self.ln(x)\n",
        "        logits = (\n",
        "            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)\n",
        "        ).float()\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "C1Rq-nLAafmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided Python code defines three classes: ResidualAttentionBlock, AudioEncoder, and TextDecoder. These classes are typically used in models for audio-to-text synthesis.\n",
        "\n",
        "1. ResidualAttentionBlock: This class represents a residual attention block in the model. It is a subclass of nn.Module. The class constructor takes three arguments: n_state (the input dimension of the attention mechanism), n_head (the number of attention heads), and cross_attention (a boolean flag indicating whether cross-attention is performed). The class defines the following components:\n",
        "\n",
        "  1.1. self.attn: An instance of the MultiHeadAttention class representing self-attention.\n",
        "\n",
        "  1.2. self.attn_ln: An instance of the LayerNorm class representing layer normalization applied to the output of self-attention.\n",
        "\n",
        "  1.3. self.cross_attn (optional): An instance of the MultiHeadAttention class representing cross-attention if cross_attention is True, otherwise None.\n",
        "\n",
        "  1.4. self.cross_attn_ln (optional): An instance of the LayerNorm class representing layer normalization applied to the output of cross-attention if cross_attention is True, otherwise None.\n",
        "\n",
        "  1.5. self.mlp: A sequential neural network module consisting of linear layers and GELU activation.\n",
        "\n",
        "  1.6. self.mlp_ln: An instance of the LayerNorm class representing layer normalization applied to the output of the MLP.\n",
        "\n",
        "  The forward method performs the forward pass of the residual attention block. It takes the input tensor x, an optional auxiliary input tensor xa, an optional mask tensor mask, and an optional key-value cache dictionary kv_cache. It applies self-attention (self.attn) to x, adds the result to the input tensor x, and applies layer normalization (self.attn_ln). If cross_attention is True, it performs cross-attention (self.cross_attn) using xa as the auxiliary input, adds the result to x, and applies layer normalization (self.cross_attn_ln). Finally, it applies the MLP (self.mlp) to x, adds the result to x, and applies layer normalization (self.mlp_ln). The output tensor x is returned.\n",
        "\n",
        "2. AudioEncoder: This class represents the audio encoder in the model. It is a subclass of nn.Module. The class constructor takes five arguments: n_mels (the number of mel spectrogram channels), n_ctx (the maximum sequence length), n_state (the dimension of the hidden state), n_head (the number of attention heads), and n_layer (the number of residual attention blocks). The class defines the following components:\n",
        "\n",
        "  2.1. self.conv1: An instance of the Conv1d class representing the first convolutional layer applied to the mel spectrogram.\n",
        "\n",
        "  2.2. self.conv2: An instance of the Conv1d class representing the second convolutional layer applied to the output of the first convolutional layer.\n",
        "\n",
        "  2.3. self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state)): A buffer tensor containing positional embeddings generated using the sinusoids function.\n",
        "\n",
        "  2.4. self.blocks: A module list containing n_layer instances of the ResidualAttentionBlock class.\n",
        "\n",
        "  2.5. self.ln_post: An instance of the LayerNorm class representing layer normalization applied to the output of the residual attention blocks.\n",
        "\n",
        "  The forward method performs the forward pass of the audio encoder. It takes the input tensor x representing the mel spectrogram. It applies the first convolutional layer (self.conv1), the second convolutional layer (self.conv2), and permutes the dimensions of the tensor. It checks if the shape of the tensor matches the shape of the positional embeddings and adds the positional embeddings to the tensor. Then, it iterates through the residual attention blocks (self.blocks) and applies each block to the tensor. Finally, it applies layer normalization (self.ln_post) to the tensor and returns the result.\n",
        "\n",
        "3. TextDecoder: This class represents the text decoder in the model. It is a subclass of nn.Module. The class constructor takes five arguments: n_vocab (the number of vocabulary tokens), n_ctx (the maximum sequence length), n_state (the dimension of the hidden state), n_head (the number of attention heads), and n_layer (the number of residual attention blocks). The class defines the following components:\n",
        "\n",
        "  3.1. self.token_embedding: An embedding layer for the text tokens.\n",
        "\n",
        "  3.2. self.positional_embedding: A trainable parameter representing the positional embeddings for the text tokens.\n",
        "\n",
        "  3.3. self.blocks: A module list containing n_layer instances of the ResidualAttentionBlock class with cross_attention set to True.\n",
        "\n",
        "  3.4. self.ln: An instance of the LayerNorm class representing layer normalization applied to the output of the residual attention blocks.\n",
        "\n",
        "  3.5. self.register_buffer(\"mask\", mask, persistent=False): A buffer tensor containing a triangular mask used in the self-attention mechanism.\n",
        "\n",
        "  The forward method performs the forward pass of the text decoder. It takes the input tensor x representing the text tokens, the auxiliary input tensor xa representing the encoded audio features, and an optional key-value cache dictionary kv_cache. It applies token embedding (self.token_embedding) and positional embedding (self.positional_embedding) to the text tokens. It then iterates through the residual attention blocks (self.blocks) and applies each block to the tensor, using the auxiliary input xa and the mask tensor self.mask. After the residual attention blocks, it applies layer normalization (self.ln) to the tensor. Finally, it computes the logits by multiplying the tensor with the transposed weight matrix of the token embedding (self.token_embedding.weight) and returns the logits."
      ],
      "metadata": {
        "id": "oTtDOkyEa7IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Whisper(nn.Module):\n",
        "    def __init__(self, dims: ModelDimensions):\n",
        "        super().__init__()\n",
        "        self.dims = dims\n",
        "        self.encoder = AudioEncoder(\n",
        "            self.dims.n_mels,\n",
        "            self.dims.n_audio_ctx,\n",
        "            self.dims.n_audio_state,\n",
        "            self.dims.n_audio_head,\n",
        "            self.dims.n_audio_layer,\n",
        "        )\n",
        "        self.decoder = TextDecoder(\n",
        "            self.dims.n_vocab,\n",
        "            self.dims.n_text_ctx,\n",
        "            self.dims.n_text_state,\n",
        "            self.dims.n_text_head,\n",
        "            self.dims.n_text_layer,\n",
        "        )\n",
        "        # use the last half among the decoder layers for time alignment by default;\n",
        "        # to use a specific set of heads, see `set_alignment_heads()` below.\n",
        "        all_heads = torch.zeros(\n",
        "            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n",
        "        )\n",
        "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
        "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
        "\n",
        "    def set_alignment_heads(self, dump: bytes):\n",
        "        array = np.frombuffer(\n",
        "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
        "        ).copy()\n",
        "        mask = torch.from_numpy(array).reshape(\n",
        "            self.dims.n_text_layer, self.dims.n_text_head\n",
        "        )\n",
        "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
        "\n",
        "    def embed_audio(self, mel: torch.Tensor):\n",
        "        return self.encoder(mel)\n",
        "\n",
        "    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):\n",
        "        return self.decoder(tokens, audio_features)\n",
        "\n",
        "    def forward(\n",
        "        self, mel: torch.Tensor, tokens: torch.Tensor\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        return self.decoder(tokens, self.encoder(mel))\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    @property\n",
        "    def is_multilingual(self):\n",
        "        return self.dims.n_vocab >= 51865\n",
        "\n",
        "    @property\n",
        "    def num_languages(self):\n",
        "        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n",
        "\n",
        "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
        "        \"\"\"\n",
        "        The `MultiHeadAttention` module optionally accepts `kv_cache` which stores the key and value\n",
        "        tensors calculated for the previous positions. This method returns a dictionary that stores\n",
        "        all caches, and the necessary hooks for the key and value projection modules that save the\n",
        "        intermediate tensors to be reused during later calculations.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cache : Dict[nn.Module, torch.Tensor]\n",
        "            A dictionary object mapping the key/value projection modules to its cache\n",
        "        hooks : List[RemovableHandle]\n",
        "            List of PyTorch RemovableHandle objects to stop the hooks to be called\n",
        "        \"\"\"\n",
        "        cache = {**cache} if cache is not None else {}\n",
        "        hooks = []\n",
        "\n",
        "        def save_to_cache(module, _, output):\n",
        "            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n",
        "                # save as-is, for the first token or cross attention\n",
        "                cache[module] = output\n",
        "            else:\n",
        "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
        "            return cache[module]\n",
        "\n",
        "        def install_hooks(layer: nn.Module):\n",
        "            if isinstance(layer, MultiHeadAttention):\n",
        "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
        "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
        "\n",
        "        self.decoder.apply(install_hooks)\n",
        "        return cache, hooks\n",
        "\n",
        "    detect_language = detect_language_function\n",
        "    transcribe = transcribe_function\n",
        "    decode = decode_function"
      ],
      "metadata": {
        "id": "1KBBvPcbasxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided Python code defines a class called Whisper, which is a subclass of nn.Module. This class represents a Whisper model used for speech synthesis, specifically converting audio input into text output.\n",
        "\n",
        "Here is a breakdown of the code:\n",
        "\n",
        "1. Initialization:\n",
        "\n",
        "  1.1. The __init__ method takes an input argument dims, which is an instance of the ModelDimensions class. This class contains various dimensions and parameters related to the model.\n",
        "\n",
        "  1.2. The method initializes the parent class nn.Module using super().__init__().\n",
        "\n",
        "  1.3. It assigns the dims argument to an instance variable self.dims for later use.\n",
        "\n",
        "  1.4. It creates an instance of the AudioEncoder class, passing the necessary dimensions from dims, and assigns it to self.encoder.\n",
        "\n",
        "  1.5. It creates an instance of the TextDecoder class, passing the necessary dimensions from dims, and assigns it to self.decoder.\n",
        "\n",
        "  1.6. It initializes a tensor alignment_heads with shape (n_text_layer, n_text_head) where n_text_layer and n_text_head are dimensions from dims. The tensor is initialized with all False values except for the last half of the n_text_layer which is set to True. This tensor is registered as a buffer using self.register_buffer() and assigned to self.alignment_heads.\n",
        "\n",
        "2. Setting Alignment Heads:\n",
        "\n",
        "  2.1. The set_alignment_heads method takes a byte string dump as input.\n",
        "\n",
        "  2.2. It decompresses the byte string using gzip and decodes it using base85.\n",
        "\n",
        "  2.3. It converts the resulting array into a boolean array and reshapes it to (n_text_layer, n_text_head).\n",
        "\n",
        "  2.4. It registers the reshaped array as a buffer using self.register_buffer() and assigns it to self.alignment_heads.\n",
        "\n",
        "3. Audio Embedding:\n",
        "\n",
        "  3.1. The embed_audio method takes a tensor mel representing mel spectrogram as input.\n",
        "\n",
        "  3.2. It passes the mel tensor through the self.encoder and returns the result.\n",
        "\n",
        "4. Logits Calculation:\n",
        "\n",
        "  4.1. The logits method takes two tensors tokens and audio_features as input.\n",
        "\n",
        "  4.2. It passes the tokens and audio_features tensors through the self.decoder and returns the result.\n",
        "\n",
        "5. Forward Pass:\n",
        "\n",
        "  5.1. The forward method takes two tensors mel and tokens as input.\n",
        "\n",
        "  5.2. It passes the mel tensor through the self.encoder to obtain audio features.\n",
        "\n",
        "  5.3. It passes the tokens and audio features through the self.decoder to obtain the output logits.\n",
        "\n",
        "  5.4. It returns a dictionary containing the output logits.\n",
        "  \n",
        "6. Properties:\n",
        "\n",
        "  6.1. The device property returns the device of the model parameters.\n",
        "\n",
        "  6.2. The is_multilingual property returns a boolean indicating whether the model supports multiple languages based on the vocabulary size.\n",
        "\n",
        "  6.3. The num_languages property returns the number of languages supported by the model based on the vocabulary size.\n",
        "\n",
        "7. Key-Value Cache Hooks:\n",
        "\n",
        "  7.1. The install_kv_cache_hooks method is used to install hooks for caching intermediate tensors during the key-value projection in the MultiHeadAttention module.\n",
        "\n",
        "  7.2. It takes an optional cache dictionary as input, which stores the key-value caches.\n",
        "\n",
        "  7.3. It initializes an empty dictionary cache if cache is None.\n",
        "\n",
        "  7.4. It defines two nested functions: save_to_cache and install_hooks.\n",
        "\n",
        "  7.5. The save_to_cache function is a hook that saves the intermediate tensors to the cache dictionary.\n",
        "\n",
        "  7.6. The install_hooks function is used to traverse through the self.decoder and install hooks for the key and value projection modules.\n",
        "\n",
        "  7.7. The method applies the install_hooks function to the self.decoder module and returns the cache dictionary and a list of hooks.\n",
        "\n",
        "The remaining part of the code includes references to external functions (detect_language, transcribe, decode) which are not provided in the code snippet. These functions are likely defined elsewhere and serve specific purposes related to the speech synthesis model."
      ],
      "metadata": {
        "id": "VlDs5qxBb6re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wav2Vec2"
      ],
      "metadata": {
        "id": "_rkHZkPbml7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2Vec2 represents a groundbreaking approach to Automatic Speech Recognition (ASR) developed by the Hugging Face and Facebook AI teams. It's a neural network architecture designed to transcribe speech into text, focusing on self-supervised learning methods to leverage unlabeled audio data efficiently.\n",
        "\n",
        "Unlike traditional ASR models that heavily rely on supervised learning with paired audio-text data, Wav2Vec2 employs a self-supervised learning paradigm. It learns from raw audio data without requiring aligned transcripts for training. This is achieved through a process known as Contrastive Predictive Coding (CPC), where the model predicts future audio frames from preceding ones within the same audio clip.\n",
        "\n",
        "The core innovation of Wav2Vec2 lies in its ability to generate representations, called contextualized speech representations, capturing meaningful information from raw audio. It uses convolutional neural networks (CNNs) for feature extraction, hierarchical quantization, and a transformer-based architecture for context aggregation.\n",
        "\n",
        "These contextualized speech representations are powerful in capturing high-level features from audio, enabling the model to understand phonetic nuances, prosody, and language-specific speech patterns. This leads to better generalization and adaptation to various accents and speaking styles, making it more robust and versatile in diverse linguistic contexts.\n",
        "\n",
        "Wav2Vec2 has demonstrated impressive results in various speech-related tasks, including speech recognition, speaker identification, and voice activity detection. It has also facilitated the development of language models that integrate both text and speech modalities, further advancing multimodal AI applications.\n",
        "\n",
        "In this research we used pretrained Wav2Vec2 model and finetuned it not only on competition data, but also on datasets like Shrutilipi, MADASR and ULCA.\n",
        "\n",
        "Once again, we aim on analyzing the code we produced step-by-step."
      ],
      "metadata": {
        "id": "vhUbRzc4mt6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from utils import *\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    data_path = \"/path_to_training_data\"\n",
        "\n",
        "    # download and save pretrained\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\n",
        "        \"ai4bharat/indicwav2vec_v1_bengali\"\n",
        "    )\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(\"ai4bharat/indicwav2vec_v1_bengali\")\n",
        "\n",
        "    processor.save_pretrained(\"pretrained/ai4bharat/indicwav2vec_v1_bengali/\")\n",
        "    model.save_pretrained(\"pretrained/ai4bharat/indicwav2vec_v1_bengali/\")\n",
        "\n",
        "    train_df = pd.read_csv(os.path.join(data_path, \"train_comp_processed.csv\"))\n",
        "    train_shru = pd.read_csv(os.path.join(data_path, \"train_shru_processed.csv\"))\n",
        "    train_respin = pd.read_csv(os.path.join(data_path, \"train_respin_processed.csv\"))\n",
        "    train_ulca = pd.read_csv(os.path.join(data_path, \"train_ulca_processed.csv\"))\n",
        "\n",
        "    all_data = pd.concat([train_df, train_shru, train_respin, train_ulca])\n",
        "    all_data = all_data.dropna(subset=['sentence'])\n",
        "\n",
        "    texts = all_data[\"sentence\"].tolist()\n",
        "    vocab_list = []\n",
        "    for text in texts:\n",
        "        vocab_list.extend(list(text))\n",
        "    vocab_list = list(set(vocab_list))\n",
        "\n",
        "    old_vocab = json.load(\n",
        "        open(\"pretrained/ai4bharat/indicwav2vec_v1_bengali/vocab.json\", \"rb\")\n",
        "    )\n",
        "    new_vocab = list(set(vocab_list) - set(old_vocab.keys()) - set([\" \"]))\n",
        "\n",
        "    len_old_vocab = len(old_vocab)\n",
        "    for k in range(0, len(new_vocab)):\n",
        "        old_vocab[new_vocab[k]] = k + len_old_vocab\n",
        "\n",
        "    print(len_old_vocab, len(old_vocab))\n",
        "    print(new_vocab)\n",
        "\n",
        "    vocab_dict = json.dumps(old_vocab, ensure_ascii=False)\n",
        "    with open(\n",
        "        \"pretrained/ai4bharat/indicwav2vec_v1_bengali/vocab.json\", \"w\"\n",
        "    ) as fp:\n",
        "        fp.write(vocab_dict)"
      ],
      "metadata": {
        "id": "VxLHX9jrm4-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python script focuses on utilizing a pretrained model called \"indicwav2vec_v1_bengali\" from the AI4Bharat project, specifically for Bengali speech processing. It is the ASR Wav2Vec2 model we introduced earlier.\n",
        "\n",
        "Let's break down the code:\n",
        "\n",
        "1. Imports: The script imports necessary libraries/modules like re, os, utils, pandas, json, and components from the transformers library (Wav2Vec2CTCTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC).\n",
        "\n",
        "2. Pretrained Model Initialization: It initializes a Wav2Vec2 processor and model using the \"ai4bharat/indicwav2vec_v1_bengali\" pretrained weights and configurations.\n",
        "\n",
        "3. Saving Pretrained Models: The script saves the initialized processor and model to a specified directory for future use.\n",
        "\n",
        "4. Data Loading: It loads training data from multiple CSV files (train_comp_processed.csv, train_shru_processed.csv, train_respin_processed.csv, train_ulca_processed.csv) located in the specified data path.\n",
        "\n",
        "5. Data Concatenation and Cleaning: It concatenates all the loaded dataframes into a single dataframe (all_data). It then drops rows with missing values in the 'sentence' column.\n",
        "\n",
        "6. Vocabulary Preparation: It prepares a list of unique characters (vocab_list) from the 'sentence' column in the concatenated dataframe. It compares this list with the existing vocabulary (loaded from a JSON file) to extend the vocabulary with new characters encountered in the dataset.\n",
        "\n",
        "7. Updating Vocabulary: It extends the existing vocabulary with new characters and assigns unique indices to these new characters.\n",
        "\n",
        "8. Vocabulary Update and Saving: Finally, it updates the JSON file containing the vocabulary with the extended vocabulary and saves it for future use."
      ],
      "metadata": {
        "id": "rudlCjiWoUCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Config, Wav2Vec2ConformerForCTC\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        ")\n",
        "\n",
        "class Wav2Vec2ForCTCV2(Wav2Vec2ForCTC):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def resize_lm_head(self, new_num_tokens):\n",
        "        old_lm_head = self.lm_head\n",
        "        # Build new lm head\n",
        "        old_num_tokens, old_lm_head_dim = old_lm_head.weight.size()\n",
        "        new_lm_head_shape = (old_lm_head_dim, new_num_tokens)\n",
        "        has_new_lm_head_bias = old_lm_head.bias is not None\n",
        "        new_lm_head = nn.Linear(*new_lm_head_shape, bias=has_new_lm_head_bias)\n",
        "        new_lm_head = new_lm_head.to(\n",
        "            old_lm_head.weight.device, dtype=old_lm_head.weight.dtype\n",
        "        )\n",
        "        self._init_weights(new_lm_head)\n",
        "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
        "\n",
        "        # initialize new lm head (in particular added tokens)\n",
        "        self._init_weights(new_lm_head)\n",
        "\n",
        "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
        "        new_lm_head.weight.data[:num_tokens_to_copy, :] = old_lm_head.weight.data[\n",
        "            :num_tokens_to_copy, :\n",
        "        ]\n",
        "        new_lm_head.bias.data[:num_tokens_to_copy] = old_lm_head.bias.data[\n",
        "            :num_tokens_to_copy\n",
        "        ]\n",
        "        self.lm_head = new_lm_head\n",
        "\n",
        "\n",
        "class Wav2Vec2ForCTCV3(Wav2Vec2ForCTC):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        output_hidden_size = (\n",
        "            config.output_hidden_size if hasattr(config, \"add_adapter\") and config.add_adapter else config.hidden_size\n",
        "        )\n",
        "        self.lm_head_inter = nn.Linear(output_hidden_size, config.vocab_size)\n",
        "        # self.dropout_inter = nn.Dropout(config.final_dropout)\n",
        "\n",
        "    def resize_lm_head(self, new_num_tokens):\n",
        "        old_lm_head = self.lm_head\n",
        "        # Build new lm head\n",
        "        old_num_tokens, old_lm_head_dim = old_lm_head.weight.size()\n",
        "        new_lm_head_shape = (old_lm_head_dim, new_num_tokens)\n",
        "        has_new_lm_head_bias = old_lm_head.bias is not None\n",
        "        new_lm_head = nn.Linear(*new_lm_head_shape, bias=has_new_lm_head_bias)\n",
        "        new_lm_head = new_lm_head.to(\n",
        "            old_lm_head.weight.device, dtype=old_lm_head.weight.dtype\n",
        "        )\n",
        "        self._init_weights(new_lm_head)\n",
        "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
        "\n",
        "        # initialize new lm head (in particular added tokens)\n",
        "        self._init_weights(new_lm_head)\n",
        "\n",
        "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
        "        new_lm_head.weight.data[:num_tokens_to_copy, :] = old_lm_head.weight.data[\n",
        "            :num_tokens_to_copy, :\n",
        "        ]\n",
        "        new_lm_head.bias.data[:num_tokens_to_copy] = old_lm_head.bias.data[\n",
        "            :num_tokens_to_copy\n",
        "        ]\n",
        "        self.lm_head = new_lm_head\n",
        "        self.lm_head_inter = new_lm_head\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values: Optional[torch.Tensor],\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n",
        "            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n",
        "            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n",
        "            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n",
        "            config.vocab_size - 1]`.\n",
        "        \"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.wav2vec2(\n",
        "            input_values,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        #print(type(outputs), len(outputs[0]), len(outputs[1]), len(outputs[2]))\n",
        "        hidden_states_inter = outputs[2][12]\n",
        "        logits_inter = self.lm_head_inter(hidden_states_inter)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if labels.max() >= self.config.vocab_size:\n",
        "                raise ValueError(f\"Label values must be <= vocab_size: {self.config.vocab_size}\")\n",
        "\n",
        "            # retrieve loss input_lengths from attention_mask\n",
        "            attention_mask = (\n",
        "                attention_mask if attention_mask is not None else torch.ones_like(input_values, dtype=torch.long)\n",
        "            )\n",
        "            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
        "\n",
        "            # assuming that padded tokens are filled with -100\n",
        "            # when not being attended to\n",
        "            labels_mask = labels >= 0\n",
        "            target_lengths = labels_mask.sum(-1)\n",
        "            flattened_targets = labels.masked_select(labels_mask)\n",
        "\n",
        "            # ctc_loss doesn't support fp16\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n",
        "            log_probs_inter = nn.functional.log_softmax(logits_inter, dim=-1, dtype=torch.float32).transpose(0, 1)\n",
        "\n",
        "            with torch.backends.cudnn.flags(enabled=False):\n",
        "                loss = nn.functional.ctc_loss(\n",
        "                    log_probs,\n",
        "                    flattened_targets,\n",
        "                    input_lengths,\n",
        "                    target_lengths,\n",
        "                    blank=self.config.pad_token_id,\n",
        "                    reduction=self.config.ctc_loss_reduction,\n",
        "                    zero_infinity=self.config.ctc_zero_infinity,\n",
        "                )\n",
        "                loss_inter = nn.functional.ctc_loss(\n",
        "                    log_probs_inter,\n",
        "                    flattened_targets,\n",
        "                    input_lengths,\n",
        "                    target_lengths,\n",
        "                    blank=self.config.pad_token_id,\n",
        "                    reduction=self.config.ctc_loss_reduction,\n",
        "                    zero_infinity=self.config.ctc_zero_infinity,\n",
        "                )\n",
        "                loss = 0.7*loss + 0.3*loss_inter\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return CausalLMOutput(\n",
        "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
        "        )"
      ],
      "metadata": {
        "id": "p6qXQ5e9po55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines two custom classes, Wav2Vec2ForCTCV2 and Wav2Vec2ForCTCV3, which extend the base class Wav2Vec2ForCTC from the Hugging Face transformers library. These classes introduce modifications and additional functionality to the Wav2Vec2 model for connectionist temporal classification (CTC).\n",
        "\n",
        "Wav2Vec2ForCTCV2 includes a method resize_lm_head that adjusts the linear layer in the model responsible for language modeling (lm_head). This adjustment involves copying weights from the existing lm_head to a new linear layer with a potentially different number of tokens. This method allows for adapting the model's output to different vocabulary sizes.\n",
        "\n",
        "Wav2Vec2ForCTCV3 adds another modification to the base Wav2Vec2ForCTC class. In addition to resizing the lm_head, it introduces a new linear layer called lm_head_inter to handle intermediate language modeling. This class further extends the model's capability to adapt to different vocabularies and potentially introduces an intermediate language modeling stage during the forward pass.\n",
        "\n",
        "Both classes inherit the forward method from the base class, which performs the main operations during the model's forward pass. This includes computing logits from the model's output, handling CTC loss computation, and returning the loss, logits, hidden states, and attentions if required.\n",
        "\n",
        "The code also involves handling different token vocabularies and lengths, adjusting the model's output to accommodate varying token sizes, and computing CTC loss for training the model on speech-to-text tasks. Additionally, it ensures compatibility with different configurations and settings for the Wav2Vec2 model."
      ],
      "metadata": {
        "id": "105w7n3bqLNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import ast\n",
        "import librosa\n",
        "import glob\n",
        "import csv\n",
        "import pickle\n",
        "from datasets import Audio\n",
        "from datasets import Dataset\n",
        "from bnunicodenormalizer import Normalizer\n",
        "bnorm=Normalizer()\n",
        "from datasets import concatenate_datasets\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chars_to_ignore_regex = '[\\\\,\\?\\!\\;\\:\\\"\\\\\\'\\\\\\\\]'\n",
        "data_path = \"/path_to_training_data\"\n",
        "audio_dir = data_path + \"/train_mp3s/\"\n",
        "num_workers = 16\n",
        "\n",
        "def get_audio_length(audio_path):\n",
        "    w = librosa.load(audio_path, sr=16_000, mono=False)[0]\n",
        "    if len(w.shape)==2:\n",
        "        print(w.shape)\n",
        "    return w.shape[0]\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub(chars_to_ignore_regex, ' ', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def normalize(text):\n",
        "    _words = [bnorm(word)['normalized']  for word in text.split()]\n",
        "    text =  \" \".join([word for word in _words if word is not None])\n",
        "    return text\n",
        "\n",
        "# Prefilter data according to hosts' recommendation\n",
        "df_qa = pd.read_csv(os.path.join(data_path, \"NISQA_wavfiles.csv\"),sep=\",\")\n",
        "df_qa.rename(columns={'deg':'id'},inplace=True) ## rename to match other dfs\n",
        "df_qa['id'] = df_qa['id'].apply(lambda x:x.split('.')[0])  ## remove .wav\n",
        "df_qa = df_qa[df_qa.mos_pred>1.5]\n",
        "df = pd.read_csv(os.path.join(data_path, \"train_metadata_corrected.csv\"),sep=\",\")\n",
        "df[\"path\"] = df['id'].apply(lambda x:audio_dir+x+\".mp3\")\n",
        "df.set_index('id',inplace=True)\n",
        "df = df.join(df_qa.set_index('id'), how='inner')\n",
        "df = df.dropna(subset='yellowking_preds')[df.ykg_wer < 3]\n",
        "\n",
        "df[\"sentence\"] = [ normalize(x) for x in tqdm(df[\"sentence\"]) ]\n",
        "df[\"sentence\"] = [ remove_special_characters(x) for x in tqdm(df[\"sentence_p\"]) ]\n",
        "df[\"length\"] = [ get_audio_length(x) for x in tqdm(df[\"path\"]) ]\n",
        "\n",
        "df = df[[\"path\", \"sentence\", \"length\"]]\n",
        "df_meta.to_csv(os.path.join(data_path, \"train_comp_processed.csv\"), index=False)"
      ],
      "metadata": {
        "id": "JA5BLRvvqX_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the code above do?\n",
        "\n",
        "1. Functions:\n",
        "\n",
        "  1.1. get_audio_length(audio_path): Uses librosa to load audio files and return their lengths in samples.\n",
        "\n",
        "  1.2. remove_special_characters(text): Removes specific characters (like punctuation) from text data.\n",
        "\n",
        "  1.3. normalize(text): Normalizes the text data using a Bengali Unicode normalizer.\n",
        "\n",
        "2. Preprocessing Steps:\n",
        "\n",
        "  2.1. Loading and Filtering Data:\n",
        "\n",
        "  Reads data from CSV files (NISQA_wavfiles.csv and train_metadata_corrected.csv);\n",
        "  Filters and preprocesses the data based on certain conditions, such as removing rows with low 'mos_pred' values, matching IDs, and filtering based on 'ykg_wer' and 'yellowking_preds'.\n",
        "\n",
        "  2.2. Data Normalization:\n",
        "\n",
        "  Normalizes the sentences in the DataFrame using the defined normalize function.\n",
        "\n",
        "  2.3. Removing Special Characters:\n",
        "\n",
        "  Removes specific characters (specified in the chars_to_ignore_regex pattern) from the preprocessed sentences.\n",
        "\n",
        "  2.4. Audio Length Calculation:\n",
        "\n",
        "  Calculates the lengths of audio files using the get_audio_length function.\n",
        "\n",
        "  2.5. Data Preparation and Export:\n",
        "\n",
        "  Selects specific columns (\"path\", \"sentence\", \"length\") from the DataFrame.\n",
        "  Saves the processed DataFrame as a CSV file named \"train_comp_processed.csv\" in the specified data path."
      ],
      "metadata": {
        "id": "AZEi2nThqgqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "import librosa\n",
        "import warnings\n",
        "import argparse\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from functools import partial\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import transformers\n",
        "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, AutoProcessor, AutoConfig, AutoFeatureExtractor, AutoModelForSpeechSeq2Seq, AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer, Seq2SeqTrainer, Seq2SeqTrainingArguments, HfArgumentParser, set_seed\n",
        "from datasets import load_dataset, load_metric, Audio\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Any, Union, Optional\n",
        "from models import Wav2Vec2ForCTCV2\n",
        "from audiomentations import *\n",
        "\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ[\"WANDB_PROJECT\"] = \"b_speech\"\n",
        "\n",
        "# Training config class.\n",
        "class CFG:\n",
        "    dns_noise_path = \"/path_to_DNS_challenge_noise\"\n",
        "    musan_path = \"/path_to_MUSAN\"\n",
        "\n",
        "    pretrained_path = \"pretrained/ai4bharat/indicwav2vec_v1_bengali\"\n",
        "    save_dir_stage_1 = \"ckpt_stage1\"\n",
        "    save_dir_stage_2 = \"ckpt_stage2\"\n",
        "    save_dir_stage_3 = \"ckpt_stage3\"\n",
        "\n",
        "    sample_rate = 16000\n",
        "    epochs = 5\n",
        "    lr = 4e-5\n",
        "\n",
        "    # Dropout configs for pretrained wav2vec2 model.\n",
        "    attention_dropout = 0.1\n",
        "    hidden_dropout = 0.1\n",
        "    feat_proj_dropout = 0.1\n",
        "    mask_time_prob = 0.1\n",
        "    layerdrop = 0.1\n",
        "    mask_feature_prob = 0.05\n",
        "\n",
        "    max_input_length_in_sec = 13.0\n",
        "    min_input_length_in_sec = 2.0\n",
        "\n",
        "    # Trainer arugments.\n",
        "    trainer = TrainingArguments(\n",
        "      output_dir=\"weights\",\n",
        "      group_by_length=False,\n",
        "      length_column_name=\"input_length\",\n",
        "      per_device_train_batch_size=4,\n",
        "      per_device_eval_batch_size=4,\n",
        "      gradient_accumulation_steps=1,\n",
        "      num_train_epochs=epochs,\n",
        "      gradient_checkpointing=False,\n",
        "      fp16=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=100, # number of bathes after which to log metrics from the model\n",
        "    report_to=\"wandb\",\n",
        "      learning_rate=lr,\n",
        "    weight_decay=0.0025,\n",
        "      dataloader_num_workers=16,\n",
        "    warmup_ratio=0.1,\n",
        "      save_total_limit=5,\n",
        "      push_to_hub=False,\n",
        "      load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    metric_for_best_model='eval_wer',\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "      remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "augment_read_speech = Compose([\n",
        "    TimeStretch(min_rate=0.8, max_rate=2.0, p=0.5, leave_length_unchanged=False),\n",
        "    RoomSimulator(\n",
        "        p=0.3),\n",
        "    OneOf([\n",
        "        AddBackgroundNoise(\n",
        "            sounds_path=[\n",
        "                CFG.dns_noise_path,\n",
        "            ],\n",
        "            min_snr_in_db=5.0,\n",
        "            max_snr_in_db=30.0,\n",
        "            noise_transform=PolarityInversion(),\n",
        "            p=1.0\n",
        "        ),\n",
        "        AddBackgroundNoise(\n",
        "            sounds_path=[\n",
        "                CFG.musan_path,\n",
        "            ],\n",
        "            min_snr_in_db=5.0,\n",
        "            max_snr_in_db=30.0,\n",
        "            noise_transform=PolarityInversion(),\n",
        "            p=1.0\n",
        "        ),\n",
        "        AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=1.0),\n",
        "    ], p=0.7),\n",
        "    Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.2),\n",
        "    ])\n",
        "\n",
        "augment_spontaneous_speech = Compose([\n",
        "    TimeStretch(min_rate=0.8, max_rate=1.1, p=0.3, leave_length_unchanged=False),\n",
        "    RoomSimulator(\n",
        "        p=0.3),\n",
        "    OneOf([\n",
        "        AddBackgroundNoise(\n",
        "            sounds_path=[\n",
        "                CFG.dns_noise_path,\n",
        "            ],\n",
        "            min_snr_in_db=5.0,\n",
        "            max_snr_in_db=30.0,\n",
        "            noise_transform=PolarityInversion(),\n",
        "            p=1.0\n",
        "        ),\n",
        "        AddBackgroundNoise(\n",
        "            sounds_path=[\n",
        "                CFG.musan_path,\n",
        "            ],\n",
        "            min_snr_in_db=5.0,\n",
        "            max_snr_in_db=30.0,\n",
        "            noise_transform=PolarityInversion(),\n",
        "            p=1.0\n",
        "        ),\n",
        "        AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=1.0),\n",
        "    ], p=0.3),\n",
        "    Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.2),\n",
        "    ])\n",
        "\n",
        "class BengaliDataset(Dataset):\n",
        "\n",
        "    def __init__(self, config, df, processor, split):\n",
        "        self.df = df\n",
        "        self.cfg = config\n",
        "        self.arch = config.arch\n",
        "        self.paths = df['path']\n",
        "        self.sentences = df['sentence']\n",
        "        self.sources = df['source']\n",
        "        self.lengths = df['length'].to_numpy()\n",
        "        self.len = len(self.df)\n",
        "        self.sr = 16_000\n",
        "\n",
        "        self.processor = processor\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def load_audio(self, idx):\n",
        "        idx %= len(self.df)\n",
        "        audio_path = self.paths[idx]\n",
        "        sentence = self.sentences[idx]\n",
        "        source = self.sources[idx]\n",
        "        num_frames = self.lengths[idx]\n",
        "        concat_augment=False\n",
        "\n",
        "        wav = librosa.load(audio_path, sr=self.sr, mono=False)[0]\n",
        "        wav = np.trim_zeros(wav, 'fb')\n",
        "\n",
        "        if self.split==\"train\":\n",
        "            if (num_frames<(self.cfg.max_input_length_in_sec*8000)) and (np.random.uniform() < 0.5):\n",
        "                num_frames_concat = self.cfg.max_input_length_in_sec*self.sr-num_frames\n",
        "                possible_indexes = np.where(self.lengths < num_frames_concat)[0]\n",
        "                if len(possible_indexes)>0:\n",
        "                    concat_augment=True\n",
        "                    selected_index = np.random.choice(possible_indexes)\n",
        "                    audio_path_concat = self.paths[selected_index]\n",
        "                    sentence_concat = self.sentences[selected_index]\n",
        "                    wav_concat = librosa.load(audio_path_concat, sr=self.sr, mono=False)[0]\n",
        "                    wav_concat = np.trim_zeros(wav_concat, 'fb')\n",
        "                    wav = np.concatenate((wav, wav_concat))\n",
        "                    sentence = sentence + \" \" + sentence_concat\n",
        "\n",
        "            try:\n",
        "                if source==\"spontaneous\":\n",
        "                    wav = augment_spontaneous_speech(samples=wav, sample_rate=self.sr)\n",
        "                else:\n",
        "                    wav = augment_read_speech(samples=wav, sample_rate=self.sr)\n",
        "            except:\n",
        "                print(audio_path)\n",
        "\n",
        "        wav = np.expand_dims(wav, axis=0)\n",
        "\n",
        "        input_values = self.processor(wav, sampling_rate=self.sr).input_values[0]\n",
        "\n",
        "        input_length = len(input_values)\n",
        "        with self.processor.as_target_processor():\n",
        "            labels = self.processor(sentence).input_ids\n",
        "\n",
        "        return {\n",
        "            'input_values':input_values,\n",
        "            'input_length':input_length,\n",
        "            'labels':labels\n",
        "        }\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.len:\n",
        "            raise IndexError(f'index {idx} out of range {self.len}')\n",
        "        return self.load_audio(idx)"
      ],
      "metadata": {
        "id": "0Ca97GmuqxmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a training pipeline for a speech recognition task using the Wav2Vec2 model in PyTorch and Hugging Face's Transformers library. Here's a detailed explanation:\n",
        "\n",
        "1. Libraries and Setup:\n",
        "The code imports various libraries required for audio processing, data handling, model training, and augmentation, including torch, pandas, wandb, librosa, and others.\n",
        "\n",
        "2. Configuration Class:\n",
        "The CFG class holds configuration parameters for the training process, such as paths to noise datasets, sample rate, epochs, learning rate, augmentation parameters, and training arguments for the Trainer.\n",
        "\n",
        "3. Augmentation Setup:\n",
        "Defines augmentation pipelines (augment_read_speech and augment_spontaneous_speech) using the audiomentations library to simulate various conditions like time stretching, room simulation, adding background noise, and adjusting gain.\n",
        "\n",
        "4. Custom Dataset Class:\n",
        "\n",
        "  4.1. BengaliDataset is a custom dataset class inheriting from torch.utils.data.Dataset.\n",
        "\n",
        "  4.2. It loads audio samples, performs augmentation (if training), tokenizes sentences, and prepares data for training the Wav2Vec2 model.\n",
        "\n",
        "  4.3. __len__() returns the length of the dataset.\n",
        "\n",
        "  4.4. load_audio() loads and preprocesses audio files, handling augmentation, concatenation, and tokenization.\n",
        "\n",
        "  4.5. __getitem__() returns a dictionary containing input values, input length, and labels for the model.\n",
        "\n",
        "5. Data Preprocessing and Augmentation:\n",
        "\n",
        "  5.1. Audio files are loaded using librosa, trimmed, and augmented based on specific conditions defined in the dataset class.\n",
        "\n",
        "  5.2. Tokenization and processing of sentences are performed using the Wav2Vec2 processor."
      ],
      "metadata": {
        "id": "7xh2cfGRsD_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "def main():\n",
        "    # seed\n",
        "    seed = 20\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"ASR model training.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_path\",\n",
        "        help=\"Path to folder with csv transcription file\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--stage\",\n",
        "        help=\"Training stage\",\n",
        "        type=int,\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--filter_csv_path\",\n",
        "        help=\"Path to csv file for data filtering\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.stage==2:\n",
        "        CFG.epochs = 3\n",
        "        CFG.lr = 3e-5\n",
        "        CFG.pretrained_path = CFG.save_dir_stage_1\n",
        "    elif args.stage==3:\n",
        "        CFG.epochs = 3\n",
        "        CFG.lr = 2e-5\n",
        "        CFG.pretrained_path = CFG.save_dir_stage_2\n",
        "\n",
        "    selected_cols = [\"path\",\"sentence\",\"length\"]\n",
        "\n",
        "    train_comp = pd.read_csv(os.path.join(args.data_path, f\"train_comp_processed.csv\"))\n",
        "    train_comp = train_comp[selected_cols]\n",
        "    train_comp[\"source\"] = \"read\"\n",
        "\n",
        "    ext_file = f\"train_shru_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    train_shru = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    train_shru = train_shru[selected_cols]\n",
        "    train_shru[\"source\"] = \"spontaneous\"\n",
        "\n",
        "    ext_file = f\"train_respin_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    train_respin = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    train_respin = train_respin[selected_cols]\n",
        "    train_respin[\"source\"] = \"read\"\n",
        "\n",
        "    ext_file = f\"train_ulca_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    train_ulca = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    train_ulca = train_ulca[selected_cols]\n",
        "    train_ulca[\"source\"] = \"spontaneous\"\n",
        "\n",
        "    ext_file = f\"valid_kathbath_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    valid_kathbath = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    valid_kathbath = valid_kathbath[selected_cols]\n",
        "    valid_kathbath[\"source\"] = \"read\"\n",
        "\n",
        "    train_df = pd.concat([train_comp, train_respin, train_ulca, train_shru])\n",
        "    valid_df = valid_kathbath\n",
        "\n",
        "    train_df = train_df[train_df.length> (CFG.min_input_length_in_sec * 16000)]\n",
        "    train_df = train_df[train_df.length< (CFG.max_input_length_in_sec * 16000)]\n",
        "\n",
        "    train_df = train_df.dropna(subset=['sentence'])\n",
        "    train_df = train_df.query('sentence.str.len() > 5')\n",
        "    valid_df = valid_df.dropna(subset=['sentence'])\n",
        "    valid_df = valid_df.query('sentence.str.len() > 5')\n",
        "\n",
        "    if args.filter_csv_path != \"\":\n",
        "        noise_df = pd.read_csv(args.filter_csv_path)\n",
        "        # Filter 10% noisiest data\n",
        "        noise_df = noise_df.sort_values('wer', ascending=False)[:int(noise_df.shape[0]*0.1)]\n",
        "        noise_df = noise_df[\"path\"].tolist()\n",
        "        train_df = train_df[~train_df.path.isin(noise_df)]\n",
        "\n",
        "    train_df = train_df.reset_index()\n",
        "    valid_df = valid_df.reset_index()\n",
        "    print(\"Split length: \", len(train_df), len(valid_df))\n",
        "\n",
        "    train_df = train_df[[\"path\",\"sentence\", \"length\", \"source\"]]\n",
        "    valid_df = valid_df[[\"path\",\"sentence\", \"length\", \"source\"]]\n",
        "\n",
        "    processor = Wav2Vec2Processor.from_pretrained(CFG.pretrained_path)\n",
        "\n",
        "    train_dataset = BengaliDataset(CFG, train_df, processor, \"train\")\n",
        "    valid_dataset = BengaliDataset(CFG, valid_df, processor, \"valid\")\n",
        "\n",
        "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
        "\n",
        "    wer_metric = load_metric(\"wer\")\n",
        "\n",
        "    # Loading model.\n",
        "    print(\"Loading model...\")\n",
        "    model = Wav2Vec2ForCTCV2.from_pretrained(\n",
        "        CFG.pretrained_path,\n",
        "        ignore_mismatched_sizes=True,\n",
        "        attention_dropout=CFG.attention_dropout,\n",
        "        hidden_dropout=CFG.hidden_dropout,\n",
        "        feat_proj_dropout=CFG.feat_proj_dropout,\n",
        "        mask_time_prob=CFG.mask_time_prob,\n",
        "        mask_feature_prob=CFG.mask_feature_prob,\n",
        "        layerdrop=CFG.layerdrop,\n",
        "        ctc_loss_reduction=\"mean\",\n",
        "        pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    )\n",
        "    model.config.ctc_zero_infinity = True\n",
        "\n",
        "    new_vocab_size = len(processor.tokenizer.get_vocab())\n",
        "    print(\"New vocab size: \", new_vocab_size)\n",
        "    model.resize_lm_head(new_num_tokens=len(processor.tokenizer.get_vocab()))\n",
        "    model.config.vocab_size = new_vocab_size\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data_collator=data_collator,\n",
        "        args=CFG.trainer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        tokenizer=processor.feature_extractor,\n",
        "    )\n",
        "\n",
        "    print(\"Start training...\")\n",
        "    trainer.train(\n",
        "        # \"weights/checkpoint-27036\"\n",
        "    )\n",
        "\n",
        "    if args.stage==1:\n",
        "        final_dir = CFG.save_dir_stage_1\n",
        "    elif args.stage==2:\n",
        "        final_dir = CFG.save_dir_stage_2\n",
        "    else:\n",
        "        final_dir = CFG.save_dir_stage_3\n",
        "\n",
        "    trainer.save_model(final_dir)\n",
        "    processor.save_pretrained(final_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "jtg7pNm9shSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This continuation of the code sets up the training process for an Automatic Speech Recognition (ASR) model using Wav2Vec2. Here's a step-by-step explanation:\n",
        "\n",
        "1. DataCollatorCTCWithPadding Class:\n",
        "\n",
        "  1.1. This class is a data collator responsible for dynamically padding input sequences.\n",
        "\n",
        "  1.2. It takes in a Wav2Vec2Processor and a padding strategy argument.\n",
        "\n",
        "  1.3. The __call__ method pads input and label sequences separately using the processor's pad method, creating a batch of padded sequences suitable for training.\n",
        "\n",
        "2. compute_metrics Function:\n",
        "\n",
        "  2.1. This function calculates the Word Error Rate (WER) metric for the model's predictions.\n",
        "\n",
        "  2.2. It decodes model predictions and references using the processor's batch_decode method and computes the WER using the wer_metric loaded from the datasets library.\n",
        "\n",
        "3. main Function:\n",
        "\n",
        "  3.1. Parses command-line arguments related to data paths, training stages, and filtering CSV paths.\n",
        "\n",
        "  3.2. Based on the training stage, it adjusts the configuration parameters such as epochs, learning rate, and pretrained path.\n",
        "\n",
        "  3.3. Loads and preprocesses data from CSV files, concatenating various datasets, filtering based on length and quality criteria, and resetting indexes.\n",
        "\n",
        "  3.4. Creates instances of BengaliDataset using the processed train and validation datasets.\n",
        "\n",
        "  3.5. Initializes a DataCollatorCTCWithPadding for preparing batches for the model.\n",
        "\n",
        "  3.6. Loads the Wav2Vec2 model and configures it for training, including resizing the LM head for the updated vocabulary size.\n",
        "\n",
        "  3.7. Sets up a Trainer object with necessary parameters for training, including datasets, model, data collator, evaluation metrics, and training arguments.\n",
        "\n",
        "  3.8.Initiates training using the trainer.train() method, which runs the training loop.\n",
        "\n",
        "  3.9. Saves the final model and processor based on the stage of training.\n",
        "\n",
        "4. if __name__ == \"__main__\":\n",
        "\n",
        "  4.1. Calls the main() function when the script is executed directly."
      ],
      "metadata": {
        "id": "PluPtFDNssIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing as tp\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "import pandas as pd\n",
        "import pyctcdecode\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import argparse\n",
        "\n",
        "import librosa\n",
        "\n",
        "import jiwer\n",
        "import pyctcdecode\n",
        "import kenlm\n",
        "import os\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ProcessorWithLM, Wav2Vec2ForCTC\n",
        "from bnunicodenormalizer import Normalizer\n",
        "\n",
        "SAMPLING_RATE = 16_000\n",
        "\n",
        "class BengaliSRTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_paths: list[str],\n",
        "        sampling_rate: int\n",
        "    ):\n",
        "        self.audio_paths = audio_paths\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def __len__(self,):\n",
        "        return len(self.audio_paths)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        audio_path = self.audio_paths[index]\n",
        "        sr = self.sampling_rate\n",
        "        w = librosa.load(audio_path, sr=sr, mono=False)[0]\n",
        "\n",
        "        return w\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"ASR model validation.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_path\",\n",
        "        help=\"Path to folder with csv transcription file\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_path\",\n",
        "        help=\"Path to inference model\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--filter_csv_path\",\n",
        "        help=\"Path to csv file for data filtering\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "    )\n",
        "\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(args.model_path)\n",
        "    processor = Wav2Vec2Processor.from_pretrained(args.model_path)\n",
        "\n",
        "    selected_cols = [\"path\",\"sentence\",\"length\"]\n",
        "\n",
        "    train_comp = pd.read_csv(os.path.join(args.data_path, f\"train_comp_processed.csv\"))\n",
        "    train_comp = train_comp[selected_cols]\n",
        "    train_comp[\"source\"] = \"read\"\n",
        "\n",
        "    ext_file = f\"train_shru_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    train_shru = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    train_shru = train_shru[selected_cols]\n",
        "    train_shru[\"source\"] = \"spontaneous\"\n",
        "\n",
        "    ext_file = f\"train_respin_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    train_respin = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    train_respin = train_respin[selected_cols]\n",
        "    train_respin[\"source\"] = \"read\"\n",
        "\n",
        "    ext_file = f\"train_ulca_processed.csv\"\n",
        "    print(f\"Loading {ext_file}...\")\n",
        "    train_ulca = pd.read_csv(os.path.join(args.data_path, ext_file))\n",
        "    train_ulca = train_ulca[selected_cols]\n",
        "    train_ulca[\"source\"] = \"spontaneous\"\n",
        "\n",
        "    train_df = pd.concat([train_comp, train_respin, train_ulca, train_shru])\n",
        "\n",
        "    train_df = train_df[train_df.length> (CFG.min_input_length_in_sec * 16000)]\n",
        "    train_df = train_df[train_df.length< (CFG.max_input_length_in_sec * 16000)]\n",
        "\n",
        "    train_df = train_df.dropna(subset=['sentence'])\n",
        "    train_df = train_df.query('sentence.str.len() > 5')\n",
        "    train_df = train_df.reset_index()\n",
        "\n",
        "    valid = train_df[[\"path\",\"sentence\", \"length\", \"source\"]]\n",
        "\n",
        "    valid_audio_paths = valid[\"path\"].tolist()\n",
        "\n",
        "    valid_dataset = BengaliSRTestDataset(\n",
        "        valid_audio_paths, SAMPLING_RATE\n",
        "    )\n",
        "\n",
        "    collate_func = partial(\n",
        "        processor.feature_extractor,\n",
        "        return_tensors=\"pt\", sampling_rate=SAMPLING_RATE,\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=32, shuffle=False,\n",
        "        num_workers=16, collate_fn=collate_func, drop_last=False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device(\"cpu\")\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model = model.eval()\n",
        "    model = model.half()\n",
        "\n",
        "    pred_sentence_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            x = batch[\"input_values\"]\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(True):\n",
        "                y = model(x).logits\n",
        "            y = torch.argmax(y, dim=-1)\n",
        "            y = y.detach().cpu().numpy()\n",
        "\n",
        "            for l in y:\n",
        "                sentence = processor.decode(l)\n",
        "                pred_sentence_list.append(sentence)\n",
        "\n",
        "    valid[\"pred_sentence\"] = pred_sentence_list\n",
        "    valid[\"wer\"] = [\n",
        "        jiwer.wer(s, p_s)\n",
        "        for s, p_s in tqdm(valid[[\"sentence\", \"pred_sentence\"]].values)\n",
        "    ]\n",
        "\n",
        "    valid.to_csv(args.filter_csv_path, index=False)\n",
        "    print(valid[\"wer\"].mean())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dM6yrB5XtNt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is responsible for validating an Automatic Speech Recognition (ASR) model trained using the Wav2Vec2 architecture. Here's a breakdown of the validation part:\n",
        "\n",
        "1. BengaliSRTestDataset Class:\n",
        "\n",
        "  1.1. Represents a PyTorch dataset for the validation data.\n",
        "\n",
        "  1.2. Loads audio paths and reads the audio files using librosa, returning the waveforms.\n",
        "\n",
        "2. main() Function:\n",
        "\n",
        "  2.1. Parses command-line arguments related to data paths, model path, and CSV file for data filtering.\n",
        "\n",
        "  2.2. Loads the pre-trained Wav2Vec2 model and processor using the provided model path.\n",
        "\n",
        "  2.3. Reads and preprocesses training data from various CSV files, filters based on length and quality criteria, and sets up a validation dataset.\n",
        "\n",
        "  2.4. Sets up a DataLoader for the validation dataset, using the Wav2Vec2 processor's collate function to prepare batches.\n",
        "\n",
        "  2.5. Checks for GPU availability and moves the model to the appropriate device (CPU or GPU), setting the model to evaluation mode and using mixed precision (half precision) for better performance.\n",
        "\n",
        "  2.6. Runs inference on the validation set, iterating through the DataLoader, obtaining model predictions, and decoding them using the processor.\n",
        "\n",
        "  2.7. Computes Word Error Rate (WER) using the jiwer library between ground truth sentences and predicted sentences.\n",
        "\n",
        "  2.8. Saves the WER scores to a specified CSV file and prints the mean WER."
      ],
      "metadata": {
        "id": "H-GIMZATtcsF"
      }
    }
  ]
}