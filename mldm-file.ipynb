{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":52324,"databundleVersionId":6229904,"sourceType":"competition"},{"sourceId":6502355,"sourceType":"datasetVersion","datasetId":3758763},{"sourceId":6707460,"sourceType":"datasetVersion","datasetId":3865741},{"sourceId":140325995,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Copying packages and extracting them for installation\n!cp -r ../input/python-packages2 ./\n\n# Install jiwer (Word Error Rate calculation)\n!tar xvfz ./python-packages2/jiwer.tgz\n!pip install ./jiwer/jiwer-2.3.0-py3-none-any.whl -f ./ --no-index\n\n# Install bnunicodenormalizer (Bengali Unicode normalization)\n!tar xvfz ./python-packages2/normalizer.tgz\n!pip install ./normalizer/bnunicodenormalizer-0.0.24.tar.gz -f ./ --no-index\n\n# Install pyctcdecode (CTC decoding for Wav2Vec2)\n!tar xvfz ./python-packages2/pyctcdecode.tgz\n!pip install ./pyctcdecode/attrs-22.1.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/hypothesis-6.54.4-py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/pygtrie-2.5.0.tar.gz -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n\n# # Install pypi-kenlm (KenLM language model)\n!tar xvfz ./python-packages2/pypikenlm.tgz\n!pip install ./pypikenlm/pypi-kenlm-0.1.20220713.tar.gz -f ./ --no-index --no-deps","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:29:05.485118Z","iopub.execute_input":"2023-12-24T12:29:05.485360Z","iopub.status.idle":"2023-12-24T12:30:15.813111Z","shell.execute_reply.started":"2023-12-24T12:29:05.485337Z","shell.execute_reply":"2023-12-24T12:30:15.811803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -r python-packages2 jiwer normalizer pyctcdecode pypikenlm\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:15.815009Z","iopub.execute_input":"2023-12-24T12:30:15.815346Z","iopub.status.idle":"2023-12-24T12:30:16.759086Z","shell.execute_reply.started":"2023-12-24T12:30:15.815310Z","shell.execute_reply":"2023-12-24T12:30:16.758066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Импорт необходимых библиотек\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer,Wav2Vec2Processor, Wav2Vec2ProcessorWithLM, Trainer, TrainingArguments\nimport torch\nimport os\nimport string\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom pydub import AudioSegment\nimport IPython.display as ipd\nfrom collections import Counter\nfrom functools import partial\nfrom dataclasses import dataclass, field\nfrom bnunicodenormalizer import Normalizer\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom tqdm import tqdm\nimport typing as tp  # Typing module for type hints\nfrom pathlib import Path  # For working with file paths\nimport torch\nfrom torch.utils.data import Dataset\nimport pyctcdecode\nimport kenlm\nimport cloudpickle as cpkl\n\n# Путь к папке с аудиофайлами\naudio_dir = '/kaggle/input/bengaliai-speech/examples/'\n\n# Получение списка аудиофайлов\naudio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n\nBASE_DIR = '/kaggle/input/bengaliai-speech'\ntrain_data_dir = f\"{BASE_DIR}/train_mp3s/\"  \ntest_data_dir = f\"{BASE_DIR}/test_mp3s/\" \ntrain_csv_path = f\"{BASE_DIR}/train.csv\" \ndomains = f\"{BASE_DIR}/examples/\" \nDOMAINS = os.listdir(f'{BASE_DIR}/examples')\n\n\n# Создание DataFrame\ndf = pd.DataFrame({\n    'audio_path': [os.path.join(audio_dir, file) for file in audio_files]\n})\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:16.760501Z","iopub.execute_input":"2023-12-24T12:30:16.760911Z","iopub.status.idle":"2023-12-24T12:30:33.698365Z","shell.execute_reply.started":"2023-12-24T12:30:16.760872Z","shell.execute_reply":"2023-12-24T12:30:33.697590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the train.csv file using pandas\ntrain_df = pd.read_csv(train_csv_path)\n\n# Preview the first few rows of the DataFrame\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:33.700923Z","iopub.execute_input":"2023-12-24T12:30:33.702077Z","iopub.status.idle":"2023-12-24T12:30:37.964311Z","shell.execute_reply.started":"2023-12-24T12:30:33.702037Z","shell.execute_reply":"2023-12-24T12:30:37.963441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.split.unique()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:37.965327Z","iopub.execute_input":"2023-12-24T12:30:37.965645Z","iopub.status.idle":"2023-12-24T12:30:38.043241Z","shell.execute_reply.started":"2023-12-24T12:30:37.965569Z","shell.execute_reply":"2023-12-24T12:30:38.042352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_train_samples = sum(train_df[\"split\"]==\"train\")\nn_valid_samples = sum(train_df[\"split\"]==\"valid\")\nprint(f\"Total training samples : \",n_train_samples)\nprint(f\"Total validation samples : \",n_valid_samples)\nprint(\"Validation/Train ratio : \",n_valid_samples/n_train_samples)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:38.044215Z","iopub.execute_input":"2023-12-24T12:30:38.044488Z","iopub.status.idle":"2023-12-24T12:30:38.557773Z","shell.execute_reply.started":"2023-12-24T12:30:38.044465Z","shell.execute_reply":"2023-12-24T12:30:38.556653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay so validation set is very small compared to training set. We might need to use additional data from the train set as validation set.\n","metadata":{}},{"cell_type":"code","source":"plt.bar([\"train\",\"valid\"],[n_train_samples,n_valid_samples],color = ['blue', 'yellow'])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:38.559049Z","iopub.execute_input":"2023-12-24T12:30:38.559362Z","iopub.status.idle":"2023-12-24T12:30:38.778898Z","shell.execute_reply.started":"2023-12-24T12:30:38.559335Z","shell.execute_reply":"2023-12-24T12:30:38.777957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's hear some audio files and see the corresponding text","metadata":{}},{"cell_type":"code","source":"for idx in range(1,len(train_df),99999):\n    \n    mp3_path = os.path.join(train_data_dir,train_df['id'].iloc[idx])+ \".mp3\"\n    text = train_df['sentence'].iloc[idx]\n    display(AudioSegment.from_file(mp3_path))\n    print(\"Original transcription : \",text)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:38.780160Z","iopub.execute_input":"2023-12-24T12:30:38.780870Z","iopub.status.idle":"2023-12-24T12:30:42.735836Z","shell.execute_reply.started":"2023-12-24T12:30:38.780827Z","shell.execute_reply":"2023-12-24T12:30:42.734902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the test set, we'll have data from 17 domains. They have provided a sample for each of them. Let's hear some of them.\n\n","metadata":{}},{"cell_type":"code","source":"mp3_path = f\"{BASE_DIR}/examples/Slang Profanity.mp3\"\nprint(\"Slang Profanity\")\ndisplay(AudioSegment.from_file(mp3_path))\n\nmp3_path = f\"{BASE_DIR}/examples/Telemedicine.mp3\"\nprint(\"Telemedicine\")\ndisplay(AudioSegment.from_file(mp3_path))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:42.736887Z","iopub.execute_input":"2023-12-24T12:30:42.737154Z","iopub.status.idle":"2023-12-24T12:30:44.245047Z","shell.execute_reply.started":"2023-12-24T12:30:42.737130Z","shell.execute_reply":"2023-12-24T12:30:44.244053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose some random indices for checking\nrandom_indices = [0, 10, 20, 30, 40]\n\nfor idx in random_indices:\n    row = train_df.iloc[idx]\n    audio_file_path = os.path.join(train_data_dir, f\"{row['id']}.mp3\")\n\n    # Load the audio file using librosa\n    audio, sr = librosa.load(audio_file_path, sr=None)\n\n    # Plot the waveform\n    plt.figure(figsize=(10, 4))\n    librosa.display.waveshow(audio, sr=sr)\n    plt.title(f\"Waveform - Audio File ID: {row['id']}\")\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n    plt.tight_layout()\n    plt.show()\n\n    # Plot the log Mel spectrogram\n    plt.figure(figsize=(10, 4))\n    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f\"Log Mel Spectrogram - Audio File ID: {row['id']}\")\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Mel Frequency\")\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:44.250539Z","iopub.execute_input":"2023-12-24T12:30:44.250962Z","iopub.status.idle":"2023-12-24T12:30:59.137808Z","shell.execute_reply.started":"2023-12-24T12:30:44.250926Z","shell.execute_reply":"2023-12-24T12:30:59.136854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"how many unique sentences?","metadata":{}},{"cell_type":"code","source":"print(\"Total sentences :\",len(train_df))\nprint(\"Total unique sentences : \",train_df.sentence.nunique())\nprint(\"Percentage of unique sentences ; \",train_df.sentence.nunique()/len(train_df))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:30:59.138947Z","iopub.execute_input":"2023-12-24T12:30:59.139558Z","iopub.status.idle":"2023-12-24T12:31:00.510950Z","shell.execute_reply.started":"2023-12-24T12:30:59.139529Z","shell.execute_reply":"2023-12-24T12:31:00.509972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = train_df.sentence.apply(lambda x: len(x))\nplt.xlabel(\"Sentence Length\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Sentence Length Distribuition\")\nplt.hist(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:31:00.512174Z","iopub.execute_input":"2023-12-24T12:31:00.512525Z","iopub.status.idle":"2023-12-24T12:31:01.463828Z","shell.execute_reply.started":"2023-12-24T12:31:00.512498Z","shell.execute_reply":"2023-12-24T12:31:01.462900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the sentences have length<=100\n\nLet's look at the overall vocabulary size and the most frequent words","metadata":{}},{"cell_type":"code","source":"# Тут у нас полный словарь всех слов\nvocab = {}\nfor sen in tqdm(train_df.sentence):\n    for j in sen.split(\" \"):\n        try:\n            vocab[j]+=1\n        except:\n            vocab[j]=1\nprint(\"Total words in vocabulary : \",len(vocab))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:31:01.465084Z","iopub.execute_input":"2023-12-24T12:31:01.465486Z","iopub.status.idle":"2023-12-24T12:31:05.363570Z","shell.execute_reply.started":"2023-12-24T12:31:01.465448Z","shell.execute_reply":"2023-12-24T12:31:05.362652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_vocab = sorted(vocab.items(),key = lambda kv:kv[1],reverse=True)\nsorted_vocab[:30]","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:31:05.364781Z","iopub.execute_input":"2023-12-24T12:31:05.365089Z","iopub.status.idle":"2023-12-24T12:31:05.445378Z","shell.execute_reply.started":"2023-12-24T12:31:05.365063Z","shell.execute_reply":"2023-12-24T12:31:05.444465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the first 5 transcriptions\ntranscriptions = train_df['sentence'][:5].tolist()\n\n# Convert transcriptions to lowercase\ntranscriptions_lower = [transcription.lower() for transcription in transcriptions]\n\n# Remove punctuation\ntranslator = str.maketrans(\"\", \"\", string.punctuation)\ntranscriptions_no_punct = [transcription.translate(translator) for transcription in transcriptions_lower]\n\n# Tokenization\nnltk.download('punkt')  \ntranscriptions_tokens = [word_tokenize(transcription) for transcription in transcriptions_no_punct]\n\n\nnltk.download('stopwords')  \nstop_words = set(stopwords.words('bengali'))\ntranscriptions_no_stopwords = [\n    [word for word in tokens if word not in stop_words]\n    for tokens in transcriptions_tokens\n]\n\nnltk.download('wordnet')  \nstemmer = PorterStemmer()\ntranscriptions_stemmed = [\n    [stemmer.stem(word) for word in tokens]\n    for tokens in transcriptions_no_stopwords\n]\n\n# Print the preprocessed transcriptions\nfor i, transcription in enumerate(transcriptions_stemmed):\n    print(f\"Preprocessed transcription {i+1}: {' '.join(transcription)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:31:05.446617Z","iopub.execute_input":"2023-12-24T12:31:05.447003Z","iopub.status.idle":"2023-12-24T12:31:05.633764Z","shell.execute_reply.started":"2023-12-24T12:31:05.446969Z","shell.execute_reply":"2023-12-24T12:31:05.632884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the first 100 transcriptions \ntranscriptions = train_df['sentence'].tolist()\n\n# Tokenization\nnltk.download('punkt')  # Download the Punkt tokenizer\ntranscriptions_tokens = [word_tokenize(transcription) for transcription in transcriptions]\n\n# Print the tokenized transcriptions\nfor i, transcription_tokens in enumerate(transcriptions_tokens):\n    if i%100000 == 0:\n        print(f\"Tokenized transcription {i+1}: {transcription_tokens}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:31:05.635020Z","iopub.execute_input":"2023-12-24T12:31:05.635396Z","iopub.status.idle":"2023-12-24T12:33:45.303190Z","shell.execute_reply.started":"2023-12-24T12:31:05.635361Z","shell.execute_reply":"2023-12-24T12:33:45.302174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute descriptive statistics\nsentence_lengths = [len(tokens) for tokens in transcriptions_tokens]\nmin_length = min(sentence_lengths)\nmax_length = max(sentence_lengths)\nmean_length = sum(sentence_lengths) / len(sentence_lengths)\nmedian_length = sorted(sentence_lengths)[len(sentence_lengths) // 2]\n\n# Plot the distribution of sentence lengths\nplt.figure(figsize=(10, 6))\nplt.hist(sentence_lengths, bins=50, color='skyblue', edgecolor='black')\nplt.axvline(mean_length, color='red', linestyle='dashed', linewidth=2, label='Mean')\nplt.axvline(median_length, color='green', linestyle='dashed', linewidth=2, label='Median')\nplt.xlabel('Sentence Length')\nplt.ylabel('Frequency')\nplt.title('Distribution of Sentence Lengths')\nplt.legend()\nplt.show()\n\n# Print the descriptive statistics\nprint(\"Descriptive Statistics:\")\nprint(f\"Minimum Sentence Length: {min_length}\")\nprint(f\"Maximum Sentence Length: {max_length}\")\nprint(f\"Mean Sentence Length: {mean_length:.2f}\")\nprint(f\"Median Sentence Length: {median_length}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:33:45.304613Z","iopub.execute_input":"2023-12-24T12:33:45.305319Z","iopub.status.idle":"2023-12-24T12:33:49.915575Z","shell.execute_reply.started":"2023-12-24T12:33:45.305270Z","shell.execute_reply":"2023-12-24T12:33:49.914658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of unique words in each sentence\nsentences = train_df['sentence'].tolist()\nunique_word_counts = [len(set(sentence.split())) for sentence in sentences]\n\n# Compute descriptive statistics\nmin_unique_words = min(unique_word_counts)\nmax_unique_words = max(unique_word_counts)\nmean_unique_words = sum(unique_word_counts) / len(unique_word_counts)\nmedian_unique_words = sorted(unique_word_counts)[len(unique_word_counts) // 2]\n\n# Plot the distribution of unique word counts\nplt.figure(figsize=(10, 6))\nplt.hist(unique_word_counts, bins=50, color='lightcoral', edgecolor='black')\nplt.axvline(mean_unique_words, color='red', linestyle='dashed', linewidth=2, label='Mean')\nplt.axvline(median_unique_words, color='green', linestyle='dashed', linewidth=2, label='Median')\nplt.xlabel('Number of Unique Words')\nplt.ylabel('Frequency')\nplt.title('Distribution of Unique Words in Transcriptions')\nplt.legend()\nplt.show()\n\n# Print the descriptive statistics\nprint(\"Descriptive Statistics:\")\nprint(f\"Minimum Number of Unique Words: {min_unique_words}\")\nprint(f\"Maximum Number of Unique Words: {max_unique_words}\")\nprint(f\"Mean Number of Unique Words: {mean_unique_words:.2f}\")\nprint(f\"Median Number of Unique Words: {median_unique_words}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:33:49.917218Z","iopub.execute_input":"2023-12-24T12:33:49.917654Z","iopub.status.idle":"2023-12-24T12:33:56.531094Z","shell.execute_reply.started":"2023-12-24T12:33:49.917617Z","shell.execute_reply":"2023-12-24T12:33:56.530140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Domain Analysis\n\n","metadata":{}},{"cell_type":"code","source":"for idx in np.arange(5):\n    audio_file_path = f'{domains}/{DOMAINS[idx]}'\n\n    # Load the audio file using librosa\n    audio, sr = librosa.load(audio_file_path, sr=None)\n\n    # Print DOMAIN\n    print(DOMAINS[idx])\n    ipd.display(ipd.Audio(audio, rate=sr))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:33:56.532375Z","iopub.execute_input":"2023-12-24T12:33:56.532685Z","iopub.status.idle":"2023-12-24T12:33:57.654031Z","shell.execute_reply.started":"2023-12-24T12:33:56.532659Z","shell.execute_reply":"2023-12-24T12:33:57.652953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the list of domains and their corresponding audio files\nDOMAINS = [\n    'Audiobook.wav', 'Parliament Session.wav', 'Bangladeshi TV Drama.wav',\n    'Poem Recital.wav', 'Bengali Advertisement.wav', 'Puthi Literature.wav',\n    'Cartoon.wav', 'Slang Profanity.mp3', 'Debate.wav', 'Stage Drama Jatra.wav',\n    'Indian TV Drama.wav', 'Talk Show Interview.wav', 'Movie.wav', 'Telemedicine.mp3',\n    'News Presentation.wav', 'Waz Islamic Sermon.wav', 'Online Class.wav'\n]\n\n# Visualize the audio files and play them\nfor idx in np.arange(5):\n    audio_file_path = os.path.join(domains, DOMAINS[idx])\n\n    # Load the audio file using librosa\n    audio, sr = librosa.load(audio_file_path, sr=None)\n\n    # Plot the waveform\n    plt.figure(figsize=(10, 4))\n    librosa.display.waveshow(audio, sr=sr)\n    plt.title(f'Waveform - {DOMAINS[idx]}')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude')\n    plt.show()\n\n    # Plot the spectrogram\n    plt.figure(figsize=(10, 4))\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='linear')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f'Spectrogram - {DOMAINS[idx]}')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    plt.show()\n\n    # Play the audio\n    print(f\"Audio: {DOMAINS[idx]}\")\n    ipd.display(ipd.Audio(audio, rate=sr))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:33:57.655678Z","iopub.execute_input":"2023-12-24T12:33:57.656054Z","iopub.status.idle":"2023-12-24T12:34:14.136574Z","shell.execute_reply.started":"2023-12-24T12:33:57.656021Z","shell.execute_reply":"2023-12-24T12:34:14.135069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"Public Wav2Vec2 model - no FT - inference only\nIn this notebook I am using this baseline model to understand the leaderboard. After that we will fine-tune or add some new models","metadata":{}},{"cell_type":"code","source":"# Define paths and parameters for the project\nROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nDATA = INPUT / \"bengaliai-speech\"\nTRAIN = DATA / \"train_mp3s\"\nTEST = DATA / \"test_mp3s\"\nSAMPLING_RATE = 16_000\nMODEL_PATH = INPUT / \"bengali-sr-download-public-trained-models/indicwav2vec_v1_bengali/\"\nLM_PATH = INPUT / \"bengali-sr-download-public-trained-models/wav2vec2-xls-r-300m-bengali/language_model/\"","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:14.137887Z","iopub.execute_input":"2023-12-24T12:34:14.138202Z","iopub.status.idle":"2023-12-24T12:34:14.144374Z","shell.execute_reply.started":"2023-12-24T12:34:14.138176Z","shell.execute_reply":"2023-12-24T12:34:14.143343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Wav2Vec2 model and processor\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_PATH)  # CTC instance\n# processor will be responsible for handling the audion data\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:14.145556Z","iopub.execute_input":"2023-12-24T12:34:14.145884Z","iopub.status.idle":"2023-12-24T12:34:25.577785Z","shell.execute_reply.started":"2023-12-24T12:34:14.145848Z","shell.execute_reply":"2023-12-24T12:34:25.576769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the vocabulary and a decoder\n\n# Get the vocabulary from the model's tokenizer\nvocab_dict = processor.tokenizer.get_vocab()\nprint('lENGTH OF THE VOCABULARY: ',len(vocab_dict))\nvocab_dict","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:25.578910Z","iopub.execute_input":"2023-12-24T12:34:25.579212Z","iopub.status.idle":"2023-12-24T12:34:25.589194Z","shell.execute_reply.started":"2023-12-24T12:34:25.579186Z","shell.execute_reply":"2023-12-24T12:34:25.588256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the vocabulary based on token IDs\nsorted_vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n\n# Build a CTC decoder using the sorted vocabulary and a language model\ndecoder = pyctcdecode.build_ctcdecoder(\n    list(sorted_vocab_dict.keys()),  # Vocabulary keys\n    str(LM_PATH / \"5gram.bin\"),  # Path to the language model file\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:25.590703Z","iopub.execute_input":"2023-12-24T12:34:25.590973Z","iopub.status.idle":"2023-12-24T12:34:56.107958Z","shell.execute_reply.started":"2023-12-24T12:34:25.590950Z","shell.execute_reply":"2023-12-24T12:34:56.106952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a combined processor for Wav2Vec2 model input and language model decoding\nprocessor_with_lm = Wav2Vec2ProcessorWithLM(\n    feature_extractor=processor.feature_extractor,  # Feature extractor for audio data\n    tokenizer=processor.tokenizer,  # Tokenizer for text data\n    decoder=decoder  # Decoder for converting model output to text\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:56.109366Z","iopub.execute_input":"2023-12-24T12:34:56.110312Z","iopub.status.idle":"2023-12-24T12:34:56.119446Z","shell.execute_reply.started":"2023-12-24T12:34:56.110271Z","shell.execute_reply":"2023-12-24T12:34:56.118556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BengaliSRTestDataset(Dataset):\n    # A custom dataset class for handling Bengali speech test data\n    \n    def __init__(self, audio_paths: list[str], sampling_rate: int):\n        # Constructor to initialize the dataset\n        \n        # Store the list of audio file paths\n        self.audio_paths = audio_paths\n        \n        # Store the sampling rate used for audio processing\n        self.sampling_rate = sampling_rate\n    \n    def __len__(self):\n        # Return the total number of samples in the dataset\n        return len(self.audio_paths)\n    \n    def __getitem__(self, index: int):\n        # Get a sample from the dataset given an index\n        \n        # Get the audio file path corresponding to the index\n        audio_path = self.audio_paths[index]\n        \n        # Get the sampling rate from the dataset settings\n        sr = self.sampling_rate\n        \n        # Load the audio file using librosa, specifying the desired sampling rate\n        # 'mono=False' indicates to load the audio as a multi-channel signal\n        # [0] at the end gets the audio signal (the first element of the returned tuple)\n        audio_signal = librosa.load(audio_path, sr=sr, mono=False)[0]\n        \n        # Return the loaded audio signal as the sample\n        return audio_signal","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:56.120714Z","iopub.execute_input":"2023-12-24T12:34:56.121003Z","iopub.status.idle":"2023-12-24T12:34:56.129849Z","shell.execute_reply.started":"2023-12-24T12:34:56.120977Z","shell.execute_reply":"2023-12-24T12:34:56.129052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(DATA / \"sample_submission.csv\", dtype={\"id\": str})\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:56.130941Z","iopub.execute_input":"2023-12-24T12:34:56.131207Z","iopub.status.idle":"2023-12-24T12:34:56.149837Z","shell.execute_reply.started":"2023-12-24T12:34:56.131183Z","shell.execute_reply":"2023-12-24T12:34:56.148968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_audio_paths = [str(TEST / f\"{aid}.mp3\") for aid in test[\"id\"].values]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:56.155335Z","iopub.execute_input":"2023-12-24T12:34:56.155639Z","iopub.status.idle":"2023-12-24T12:34:56.160082Z","shell.execute_reply.started":"2023-12-24T12:34:56.155610Z","shell.execute_reply":"2023-12-24T12:34:56.159215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataset for testing using the list of test audio paths and specified sampling rate\ntest_dataset = BengaliSRTestDataset(\n    test_audio_paths, SAMPLING_RATE\n)\n\n# Define a partial function for collating samples into batches\ncollate_func = partial(\n    processor_with_lm.feature_extractor,\n    return_tensors=\"pt\", sampling_rate=SAMPLING_RATE,\n    padding=True,\n)\n\n# Create a data loader for testing\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=4, shuffle=False,\n    num_workers=4, collate_fn=collate_func, drop_last=False,\n    pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:56.161196Z","iopub.execute_input":"2023-12-24T12:34:56.161445Z","iopub.status.idle":"2023-12-24T12:34:56.169185Z","shell.execute_reply.started":"2023-12-24T12:34:56.161423Z","shell.execute_reply":"2023-12-24T12:34:56.168385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not torch.cuda.is_available():\n    device = torch.device(\"cpu\")\nelse:\n    device = torch.device(\"cuda\")\nprint(device)\n\n# attach cpu or gpu\nmodel = model.to(device)\nmodel = model.eval()\nmodel = model.half()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:34:56.170212Z","iopub.execute_input":"2023-12-24T12:34:56.170555Z","iopub.status.idle":"2023-12-24T12:35:00.356414Z","shell.execute_reply.started":"2023-12-24T12:34:56.170523Z","shell.execute_reply":"2023-12-24T12:35:00.355631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sentence_list = []  # Initialize an empty list to store predicted sentences\n\n# Perform inference without gradient computation because we are not fine tuning so we don't want to change the weights\nwith torch.no_grad():\n    for batch in tqdm(test_loader):  # Iterate through batches of test data\n        x = batch[\"input_values\"]  # Extract the input audio features from the batch\n        x = x.to(device, non_blocking=True)  # Move the input data to the device (GPU)\n        \n        # Use automatic mixed precision for faster and more memory-efficient inference\n        with torch.cuda.amp.autocast(True):\n            y = model(x).logits  # Get the model's output logits\n        del x\n        y = y.detach().cpu().numpy()  # Move the logits to the CPU and convert to a numpy array\n        \n        for l in y:  # Iterate through the logits of the batch\n            # Decode the logits into a sentence using the LM with beam search decoding\n            sentence = processor_with_lm.decode(l, beam_width=64).text\n            pred_sentence_list.append(sentence)  # Append the predicted sentence to the list","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:35:00.357542Z","iopub.execute_input":"2023-12-24T12:35:00.357911Z","iopub.status.idle":"2023-12-24T12:35:05.337258Z","shell.execute_reply.started":"2023-12-24T12:35:00.357883Z","shell.execute_reply":"2023-12-24T12:35:05.336211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnorm = Normalizer()  # Create a Normalizer object for text normalization\n\ndef postprocess(sentence):\n    # Define a postprocessing function to clean up and format predicted sentences\n    \n    period_set = set([\".\", \"?\", \"!\", \"।\"])  # Set of sentence-ending punctuation\n    \n    # Split the sentence into words and apply normalization using the Normalizer\n    _words = [bnorm(word)['normalized'] for word in sentence.split() if word]\n    \n    sentence = \" \".join(_words)\n    \n    if not sentence.endswith(tuple(period_set)):\n        sentence += \"।\"\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:35:05.338685Z","iopub.execute_input":"2023-12-24T12:35:05.339018Z","iopub.status.idle":"2023-12-24T12:35:05.345614Z","shell.execute_reply.started":"2023-12-24T12:35:05.338988Z","shell.execute_reply":"2023-12-24T12:35:05.344720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp_pred_sentence_list = [\n    postprocess(s) for s in tqdm(pred_sentence_list)]","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:35:05.346740Z","iopub.execute_input":"2023-12-24T12:35:05.347027Z","iopub.status.idle":"2023-12-24T12:35:05.375453Z","shell.execute_reply.started":"2023-12-24T12:35:05.347004Z","shell.execute_reply":"2023-12-24T12:35:05.374569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"sentence\"] = pp_pred_sentence_list\n\ntest.to_csv(\"submission.csv\", index=False)\n\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:35:05.376715Z","iopub.execute_input":"2023-12-24T12:35:05.377330Z","iopub.status.idle":"2023-12-24T12:35:05.389731Z","shell.execute_reply.started":"2023-12-24T12:35:05.377280Z","shell.execute_reply":"2023-12-24T12:35:05.388857Z"},"trusted":true},"execution_count":null,"outputs":[]}]}